FROM ubuntu:22.04

# Gunakan Java 11 untuk stabilitas maksimum
ENV DEBIAN_FRONTEND=noninteractive \
    JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64 \
    SPARK_HOME=/opt/spark \
    PATH=$PATH:/opt/spark/bin:/opt/spark/sbin \
    SPARK_VERSION=3.5.7 \
    HADOOP_VERSION=hadoop3

RUN apt-get update && apt-get install -y \
    openjdk-11-jdk \
    python3 \
    python3-pip \
    python-is-python3 \
    curl \
    wget \
    sudo \
    && rm -rf /var/lib/apt/lists/*

# COPY
# COPY ./jars/spark-${SPARK_VERSION}-bin-${HADOOP_VERSION}.tgz /tmp/
# RUN tar -xzf /tmp/spark-${SPARK_VERSION}-bin-${HADOOP_VERSION}.tgz -C /opt/ && \
    # mv /opt/spark-${SPARK_VERSION}-bin-${HADOOP_VERSION} $SPARK_HOME && \
    # rm /tmp/spark-${SPARK_VERSION}-bin-${HADOOP_VERSION}.tgz

DOWNLOAD 
RUN wget -q https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-${HADOOP_VERSION}.tgz && \
   tar -xzf spark-${SPARK_VERSION}-bin-${HADOOP_VERSION}.tgz && \
   mv spark-${SPARK_VERSION}-bin-${HADOOP_VERSION} $SPARK_HOME && \
   rm spark-${SPARK_VERSION}-bin-${HADOOP_VERSION}.tgz

# Pastikan file JAR ada di folder yang sama dengan Dockerfile (atau sesuaikan path-nya)
# COPY ./jars/hadoop-aws-3.3.4.jar ${SPARK_HOME}/jars/
# COPY ./jars/aws-java-sdk-bundle-1.12.606.jar ${SPARK_HOME}/jars/
# COPY ./jars/iceberg-spark-runtime-3.5_2.12-1.5.0.jar ${SPARK_HOME}/jars/
# -------------------------

# --- Bagian Download JAR yang Kompatibel ---
# Menghapus COPY lama dan menggantinya dengan download langsung
RUN wget -q https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar -P ${SPARK_HOME}/jars/ && \
    wget -q https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar -P ${SPARK_HOME}/jars/ && \
    wget -q https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.5.0/iceberg-spark-runtime-3.5_2.12-1.5.0.jar -P ${SPARK_HOME}/jars/
# -------------------------------------------

ARG USERNAME=sparkuser
RUN groupadd --gid 1000 $USERNAME \
    && useradd --uid 1000 --gid 1000 -m -s /bin/bash $USERNAME \
    && echo "$USERNAME ALL=(ALL) NOPASSWD: ALL" >> /etc/sudoers

RUN mkdir -p ${SPARK_HOME}/logs ${SPARK_HOME}/event_logs /home/${USERNAME}/notebooks && \
    chown -R $USERNAME:$USERNAME ${SPARK_HOME} /home/${USERNAME}/notebooks

RUN pip3 install --no-cache-dir jupyterlab pyspark findspark

USER ${USERNAME}
WORKDIR /home/${USERNAME}/notebooks

EXPOSE 8888 4040 18080 8080 8081 7077

# Menjalankan Master, Worker, History Server, dan Jupyter dalam satu container
CMD bash -c "\
    $SPARK_HOME/sbin/start-master.sh && \
    $SPARK_HOME/sbin/start-worker.sh spark://$(hostname):7077 && \
    $SPARK_HOME/sbin/start-history-server.sh && \
    jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --NotebookApp.token=''"