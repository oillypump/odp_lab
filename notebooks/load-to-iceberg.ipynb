{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6edaa848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from pyiceberg.catalog import load_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca2d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet -o ../data/yellow_tripdata_2023-01.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e165cb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pq.read_table(\"../data/yellow_tripdata_2023-01.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fa7620b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.Table\n",
      "VendorID: int64\n",
      "tpep_pickup_datetime: timestamp[us]\n",
      "tpep_dropoff_datetime: timestamp[us]\n",
      "passenger_count: double\n",
      "trip_distance: double\n",
      "RatecodeID: double\n",
      "store_and_fwd_flag: string\n",
      "PULocationID: int64\n",
      "DOLocationID: int64\n",
      "payment_type: int64\n",
      "fare_amount: double\n",
      "extra: double\n",
      "mta_tax: double\n",
      "tip_amount: double\n",
      "tolls_amount: double\n",
      "improvement_surcharge: double\n",
      "total_amount: double\n",
      "congestion_surcharge: double\n",
      "airport_fee: double\n",
      "----\n",
      "VendorID: [[2,2,2,1,2,...,2,2,1,1,1],[1,2,2,2,2,...,1,1,1,2,2],...,[2,2,2,2,2,...,2,2,2,2,2],[2,2,2,2,2,...,2,2,2,2,2]]\n",
      "tpep_pickup_datetime: [[2023-01-01 00:32:10.000000,2023-01-01 00:55:08.000000,2023-01-01 00:25:04.000000,2023-01-01 00:03:48.000000,2023-01-01 00:10:29.000000,...,2023-01-02 21:16:11.000000,2023-01-02 21:56:02.000000,2023-01-02 21:04:31.000000,2023-01-02 21:13:09.000000,2023-01-02 21:45:30.000000],[2023-01-02 21:49:54.000000,2023-01-02 21:17:06.000000,2023-01-02 21:35:06.000000,2023-01-02 21:18:43.000000,2023-01-02 21:24:42.000000,...,2023-01-04 14:04:17.000000,2023-01-04 14:27:49.000000,2023-01-04 14:44:46.000000,2023-01-04 14:35:46.000000,2023-01-04 14:52:44.000000],...,[2023-01-30 20:07:47.000000,2023-01-30 20:28:57.000000,2023-01-30 19:59:53.000000,2023-01-30 20:21:42.000000,2023-01-30 20:09:59.000000,...,2023-01-10 08:10:07.000000,2023-01-10 08:51:52.000000,2023-01-10 08:13:34.000000,2023-01-10 08:29:03.000000,2023-01-10 08:49:00.000000],[2023-01-10 08:30:00.000000,2023-01-10 08:34:07.000000,2023-01-10 08:06:16.000000,2023-01-10 08:47:26.000000,2023-01-10 08:43:51.000000,...,2023-01-31 23:58:34.000000,2023-01-31 23:31:09.000000,2023-01-31 23:01:05.000000,2023-01-31 23:40:00.000000,2023-01-31 23:07:32.000000]]\n",
      "tpep_dropoff_datetime: [[2023-01-01 00:40:36.000000,2023-01-01 01:01:27.000000,2023-01-01 00:37:49.000000,2023-01-01 00:13:25.000000,2023-01-01 00:21:19.000000,...,2023-01-02 21:22:04.000000,2023-01-02 22:02:42.000000,2023-01-02 21:08:06.000000,2023-01-02 21:31:43.000000,2023-01-02 21:48:18.000000],[2023-01-02 22:23:48.000000,2023-01-02 21:41:59.000000,2023-01-02 22:00:39.000000,2023-01-02 21:24:23.000000,2023-01-02 21:51:41.000000,...,2023-01-04 14:07:51.000000,2023-01-04 14:40:33.000000,2023-01-04 15:13:24.000000,2023-01-04 14:41:59.000000,2023-01-04 15:18:58.000000],...,[2023-01-30 20:24:09.000000,2023-01-30 20:37:27.000000,2023-01-30 20:16:07.000000,2023-01-30 20:32:01.000000,2023-01-30 20:17:23.000000,...,2023-01-10 08:41:22.000000,2023-01-10 09:12:03.000000,2023-01-10 08:20:49.000000,2023-01-10 08:45:05.000000,2023-01-10 09:42:00.000000],[2023-01-10 08:38:00.000000,2023-01-10 08:41:48.000000,2023-01-10 08:24:17.000000,2023-01-10 09:08:22.000000,2023-01-10 09:18:55.000000,...,2023-02-01 00:12:33.000000,2023-01-31 23:50:36.000000,2023-01-31 23:25:36.000000,2023-01-31 23:53:00.000000,2023-01-31 23:21:56.000000]]\n",
      "passenger_count: [[1,1,1,0,1,...,1,1,1,2,1],[1,1,2,1,2,...,0,2,2,1,1],...,[1,1,2,1,1,...,null,null,null,null,null],[null,null,null,null,null,...,null,null,null,null,null]]\n",
      "trip_distance: [[0.97,1.1,2.51,1.9,1.43,...,1.59,0.74,0.9,4.3,0.5],[7.9,10.13,17.71,0.61,14.41,...,0.5,1.2,4.7,0.94,3.83],...,[1.95,1.53,3.25,2.23,1.16,...,5.84,2.66,1.22,1.7,20.95],[0.88,1.11,2.03,3.12,5.51,...,3.05,5.8,4.67,3.15,2.85]]\n",
      "RatecodeID: [[1,1,1,1,1,...,1,1,1,1,1],[1,1,2,1,1,...,1,1,1,1,1],...,[1,1,1,1,1,...,null,null,null,null,null],[null,null,null,null,null,...,null,null,null,null,null]]\n",
      "store_and_fwd_flag: [[\"N\",\"N\",\"N\",\"N\",\"N\",...,\"N\",\"N\",\"Y\",\"N\",\"N\"],[\"N\",\"N\",\"N\",\"N\",\"N\",...,\"N\",\"N\",\"N\",\"N\",\"N\"],...,[\"N\",\"N\",\"N\",\"N\",\"N\",...,null,null,null,null,null],[null,null,null,null,null,...,null,null,null,null,null]]\n",
      "PULocationID: [[161,43,48,138,107,...,233,79,68,237,234],[164,132,132,140,132,...,107,158,79,246,113],...,[263,239,237,50,237,...,37,113,246,50,141],[262,170,141,4,80,...,107,112,114,230,262]]\n",
      "DOLocationID: [[141,237,238,7,79,...,141,211,158,114,164],[173,225,164,237,129,...,234,79,236,234,52],...,[238,237,68,238,229,...,195,161,100,170,132],[236,161,43,161,72,...,48,75,239,79,143]]\n",
      "payment_type: [[2,1,1,1,1,...,1,1,1,1,1],[2,1,2,2,1,...,1,1,2,1,1],...,[1,1,1,1,1,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]]\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f475ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = load_catalog(\n",
    "    name=\"hive\",\n",
    "    **{\n",
    "        \"type\": \"hive\",\n",
    "        \"uri\": \"thrift://hive-metastore:9083\",\n",
    "        \"warehouse\": \"s3a://iceberg/lakehouse\",\n",
    "        \"s3.endpoint\": \"http://minio:9000\",\n",
    "        \"s3.access-key-id\": \"minio\",\n",
    "        \"s3.secret-access-key\": \"minio123\",\n",
    "        \"s3.path-style-access\": \"true\",\n",
    "        \"s3.region\": \"us-east-1\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef8c2899",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.create_namespace(\"bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7a63306",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "When getting information for key 'lakehouse/bronze/iceberg_nyc_taxi/metadata/00000-1447851c-aecb-49fb-bbe5-9766a76b5326.metadata.json' in bucket 'iceberg': AWS Error ACCESS_DENIED during HeadObject operation: No response body.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m table = \u001b[43mcatalog\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbronze.taxi_dataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ms3://iceberg/lakehouse/bronze/iceberg_nyc_taxi\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/werk/odp_lab/.venv/lib/python3.12/site-packages/pyiceberg/catalog/hive.py:433\u001b[39m, in \u001b[36mHiveCatalog.create_table\u001b[39m\u001b[34m(self, identifier, schema, location, partition_spec, sort_order, properties)\u001b[39m\n\u001b[32m    423\u001b[39m staged_table = \u001b[38;5;28mself\u001b[39m._create_staged_table(\n\u001b[32m    424\u001b[39m     identifier=identifier,\n\u001b[32m    425\u001b[39m     schema=schema,\n\u001b[32m   (...)\u001b[39m\u001b[32m    429\u001b[39m     properties=properties,\n\u001b[32m    430\u001b[39m )\n\u001b[32m    431\u001b[39m database_name, table_name = \u001b[38;5;28mself\u001b[39m.identifier_to_database_and_table(identifier)\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_write_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstaged_table\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstaged_table\u001b[49m\u001b[43m.\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstaged_table\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetadata_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m tbl = \u001b[38;5;28mself\u001b[39m._convert_iceberg_into_hive(staged_table)\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client \u001b[38;5;28;01mas\u001b[39;00m open_client:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/werk/odp_lab/.venv/lib/python3.12/site-packages/pyiceberg/catalog/__init__.py:1005\u001b[39m, in \u001b[36mMetastoreCatalog._write_metadata\u001b[39m\u001b[34m(metadata, io, metadata_path)\u001b[39m\n\u001b[32m   1003\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m   1004\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_write_metadata\u001b[39m(metadata: TableMetadata, io: FileIO, metadata_path: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1005\u001b[39m     \u001b[43mToOutputFile\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtable_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnew_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/werk/odp_lab/.venv/lib/python3.12/site-packages/pyiceberg/serializers.py:130\u001b[39m, in \u001b[36mToOutputFile.table_metadata\u001b[39m\u001b[34m(metadata, output_file, overwrite)\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtable_metadata\u001b[39m(metadata: TableMetadata, output_file: OutputFile, overwrite: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    124\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Write a TableMetadata instance to an output file.\u001b[39;00m\n\u001b[32m    125\u001b[39m \n\u001b[32m    126\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[33;03m        output_file (OutputFile): A custom implementation of the iceberg.io.file.OutputFile abstract base class.\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[33;03m        overwrite (bool): Where to overwrite the file if it already exists. Defaults to `False`.\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43moutput_file\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m output_stream:\n\u001b[32m    131\u001b[39m         \u001b[38;5;66;03m# We need to serialize None values, in order to dump `None` current-snapshot-id as `-1`\u001b[39;00m\n\u001b[32m    132\u001b[39m         exclude_none = \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m Config().get_bool(\u001b[33m\"\u001b[39m\u001b[33mlegacy-current-snapshot-id\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    134\u001b[39m         json_bytes = metadata.model_dump_json(exclude_none=exclude_none).encode(UTF8)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/werk/odp_lab/.venv/lib/python3.12/site-packages/pyiceberg/io/pyarrow.py:366\u001b[39m, in \u001b[36mPyArrowFile.create\u001b[39m\u001b[34m(self, overwrite)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create a writable pyarrow.lib.NativeFile for this PyArrowFile's location.\u001b[39;00m\n\u001b[32m    348\u001b[39m \n\u001b[32m    349\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    363\u001b[39m \u001b[33;03m    truncate the contents of the existing file when opening the output stream.\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m overwrite \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    367\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot create file, already exists: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.location\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    368\u001b[39m     output_file = \u001b[38;5;28mself\u001b[39m._filesystem.open_output_stream(\u001b[38;5;28mself\u001b[39m._path, buffer_size=\u001b[38;5;28mself\u001b[39m._buffer_size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/werk/odp_lab/.venv/lib/python3.12/site-packages/pyiceberg/io/pyarrow.py:312\u001b[39m, in \u001b[36mPyArrowFile.exists\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check whether the location exists.\"\"\"\u001b[39;00m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_file_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# raises FileNotFoundError if it does not exist\u001b[39;00m\n\u001b[32m    313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/werk/odp_lab/.venv/lib/python3.12/site-packages/pyiceberg/io/pyarrow.py:294\u001b[39m, in \u001b[36mPyArrowFile._file_info\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Retrieve a pyarrow.fs.FileInfo object for the location.\u001b[39;00m\n\u001b[32m    288\u001b[39m \n\u001b[32m    289\u001b[39m \u001b[33;03mRaises:\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[33;03m    PermissionError: If the file at self.location cannot be accessed due to a permission error such as\u001b[39;00m\n\u001b[32m    291\u001b[39m \u001b[33;03m        an AWS error code 15.\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     file_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_filesystem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_file_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    296\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e.errno == \u001b[32m13\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mAWS Error [code 15]\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/werk/odp_lab/.venv/lib/python3.12/site-packages/pyarrow/_fs.pyx:616\u001b[39m, in \u001b[36mpyarrow._fs.FileSystem.get_file_info\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/werk/odp_lab/.venv/lib/python3.12/site-packages/pyarrow/error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/werk/odp_lab/.venv/lib/python3.12/site-packages/pyarrow/error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mOSError\u001b[39m: When getting information for key 'lakehouse/bronze/iceberg_nyc_taxi/metadata/00000-1447851c-aecb-49fb-bbe5-9766a76b5326.metadata.json' in bucket 'iceberg': AWS Error ACCESS_DENIED during HeadObject operation: No response body."
     ]
    }
   ],
   "source": [
    "table = catalog.create_table(\n",
    "    \"bronze.taxi_dataset\",\n",
    "    schema=df.schema,\n",
    "    location=\"s3://iceberg/lakehouse/bronze/iceberg_nyc_taxi\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b292964",
   "metadata": {},
   "outputs": [],
   "source": [
    "table.append(df)\n",
    "len(table.scan().to_arrow())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731a6c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
